{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2 \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Conv2D,MaxPooling2D, Dense,Flatten\n",
    "from keras.datasets import mnist \n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('sign_mnist_train.csv')\n",
    "test = pd.read_csv('sign_mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 27455 entries, 0 to 27454\n",
      "Columns: 785 entries, label to pixel784\n",
      "dtypes: int64(785)\n",
      "memory usage: 164.4 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7172 entries, 0 to 7171\n",
      "Columns: 785 entries, label to pixel784\n",
      "dtypes: int64(785)\n",
      "memory usage: 43.0 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>107</td>\n",
       "      <td>118</td>\n",
       "      <td>127</td>\n",
       "      <td>134</td>\n",
       "      <td>139</td>\n",
       "      <td>143</td>\n",
       "      <td>146</td>\n",
       "      <td>150</td>\n",
       "      <td>153</td>\n",
       "      <td>...</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>204</td>\n",
       "      <td>203</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>155</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>156</td>\n",
       "      <td>157</td>\n",
       "      <td>156</td>\n",
       "      <td>158</td>\n",
       "      <td>158</td>\n",
       "      <td>...</td>\n",
       "      <td>69</td>\n",
       "      <td>149</td>\n",
       "      <td>128</td>\n",
       "      <td>87</td>\n",
       "      <td>94</td>\n",
       "      <td>163</td>\n",
       "      <td>175</td>\n",
       "      <td>103</td>\n",
       "      <td>135</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      3     107     118     127     134     139     143     146     150   \n",
       "1      6     155     157     156     156     156     157     156     158   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0     153  ...       207       207       207       207       206       206   \n",
       "1     158  ...        69       149       128        87        94       163   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0       206       204       203       202  \n",
       "1       175       103       135       149  \n",
       "\n",
       "[2 rows x 785 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>149</td>\n",
       "      <td>149</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>150</td>\n",
       "      <td>151</td>\n",
       "      <td>...</td>\n",
       "      <td>138</td>\n",
       "      <td>148</td>\n",
       "      <td>127</td>\n",
       "      <td>89</td>\n",
       "      <td>82</td>\n",
       "      <td>96</td>\n",
       "      <td>106</td>\n",
       "      <td>112</td>\n",
       "      <td>120</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>126</td>\n",
       "      <td>128</td>\n",
       "      <td>131</td>\n",
       "      <td>132</td>\n",
       "      <td>133</td>\n",
       "      <td>134</td>\n",
       "      <td>135</td>\n",
       "      <td>135</td>\n",
       "      <td>136</td>\n",
       "      <td>...</td>\n",
       "      <td>47</td>\n",
       "      <td>104</td>\n",
       "      <td>194</td>\n",
       "      <td>183</td>\n",
       "      <td>186</td>\n",
       "      <td>184</td>\n",
       "      <td>184</td>\n",
       "      <td>184</td>\n",
       "      <td>182</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      6     149     149     150     150     150     151     151     150   \n",
       "1      5     126     128     131     132     133     134     135     135   \n",
       "\n",
       "   pixel9  ...  pixel775  pixel776  pixel777  pixel778  pixel779  pixel780  \\\n",
       "0     151  ...       138       148       127        89        82        96   \n",
       "1     136  ...        47       104       194       183       186       184   \n",
       "\n",
       "   pixel781  pixel782  pixel783  pixel784  \n",
       "0       106       112       120       107  \n",
       "1       184       184       182       180  \n",
       "\n",
       "[2 rows x 785 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(train.info())\n",
    "\n",
    "display(test.info())\n",
    "\n",
    "display(train.head(n = 2))\n",
    "display(test.head(n = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Y = train['label']\n",
    "test_Y = test['label']\n",
    "train_X = train.drop(['label'],axis = 1)\n",
    "test_X = test.drop(['label'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X.astype('float32') / 255\n",
    "test_X = test_X.astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#display(train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_X = train_X.values.reshape(27455,784)\n",
    "test_X = test_X.values.reshape(7172,784)\n",
    "train_Y = keras.utils.to_categorical(train_Y,26)\n",
    "test_Y = keras.utils.to_categorical(test_Y,26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=128,activation=\"relu\",input_shape=(784,)))\n",
    "model.add(Dense(units=128,activation=\"relu\"))\n",
    "model.add(Dense(units=128,activation=\"relu\"))\n",
    "model.add(Dense(units=26,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "27455/27455 [==============================] - 2s 71us/step - loss: 3.1930 - accuracy: 0.0794\n",
      "Epoch 2/10\n",
      "27455/27455 [==============================] - 2s 71us/step - loss: 3.0779 - accuracy: 0.1675\n",
      "Epoch 3/10\n",
      "27455/27455 [==============================] - 2s 70us/step - loss: 2.9772 - accuracy: 0.2200\n",
      "Epoch 4/10\n",
      "27455/27455 [==============================] - 2s 78us/step - loss: 2.8614 - accuracy: 0.2577\n",
      "Epoch 5/10\n",
      "27455/27455 [==============================] - ETA: 0s - loss: 2.7203 - accuracy: 0.28 - 2s 67us/step - loss: 2.7193 - accuracy: 0.2890\n",
      "Epoch 6/10\n",
      "27455/27455 [==============================] - 2s 69us/step - loss: 2.5560 - accuracy: 0.3185\n",
      "Epoch 7/10\n",
      "27455/27455 [==============================] - 2s 73us/step - loss: 2.3873 - accuracy: 0.3508\n",
      "Epoch 8/10\n",
      "27455/27455 [==============================] - 2s 70us/step - loss: 2.2262 - accuracy: 0.3804\n",
      "Epoch 9/10\n",
      "27455/27455 [==============================] - 2s 71us/step - loss: 2.0843 - accuracy: 0.4137\n",
      "Epoch 10/10\n",
      "27455/27455 [==============================] - 2s 70us/step - loss: 1.9620 - accuracy: 0.4465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x269becc52e8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=SGD(0.001),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "model.fit(train_X,train_Y,batch_size=32,epochs=10,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 0s 37us/step\n",
      "Accuracy:  0.3775794804096222\n"
     ]
    }
   ],
   "source": [
    "accuracy = model.evaluate(x=test_X,y=test_Y,batch_size=32)\n",
    "print(\"Accuracy: \",accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 26)                3354      \n",
      "=================================================================\n",
      "Total params: 136,858\n",
      "Trainable params: 136,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "model.save('CNNmodel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class:  14\n"
     ]
    }
   ],
   "source": [
    "img = test_X[7]\n",
    "test_img = img.reshape((1,784))\n",
    "img_class = model.predict_classes(test_img)\n",
    "prediction = img_class[0]\n",
    "classname = img_class[0]\n",
    "print(\"Class: \",classname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = img.reshape((28,28))\n",
    "plt.imshow(img)\n",
    "plt.title(classname)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:  \n",
    "# capturing the image from webcam \n",
    "    cam_capture = cv2.VideoCapture(0)\n",
    "    _, image_frame = cam_capture.read()\n",
    "  # to crop required part\n",
    "    im2 = crop_image(image_frame, 300,300,300,300)\n",
    "# convert to grayscale \n",
    "     \n",
    "    image_grayscale = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
    "# blurring the image \n",
    "  \n",
    "    image_grayscale_blurred =cv2.GaussianBlur(image_grayscale, (15,15), 0)\n",
    "# resize the image to 28x28\n",
    "    im3 = cv2.resize(image_grayscale_blurred, (28,28), interpolation = cv2.INTER_AREA)\n",
    "# expand the dimensions from 28x28 to 1x28x28x1\n",
    "    im4 = np.resize(im3, (28, 28, 1))\n",
    "    im5 = np.expand_dims(im4, axis=0)\n",
    "    \n",
    "    cv2.imshow(\"Image2\",im2)\n",
    "    cv2.imshow(\"Image4\",im4)\n",
    "    cv2.imshow(\"Image3\",im3)\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "    \n",
    "\n",
    "cam_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import load_model\n",
    "from skimage.transform import resize, pyramid_reduce\n",
    "\n",
    "\n",
    "model = load_model('CNNmodel.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 0.9923934\n",
      "6 1.0\n",
      "6 1.0\n",
      "6 1.0\n",
      "6 1.0\n",
      "6 1.0\n",
      "6 1.0\n",
      "6 1.0\n",
      "6 1.0\n",
      "6 1.0\n",
      "6 1.0\n",
      "6 1.0\n",
      "6 1.0\n",
      "6 1.0\n",
      "6 1.0\n",
      "6 1.0\n",
      "6 1.0\n",
      "6 1.0\n",
      "15 1.0\n",
      "15 0.9999094\n",
      "15 1.0\n",
      "15 1.0\n",
      "15 0.9998697\n",
      "15 0.9999\n"
     ]
    }
   ],
   "source": [
    "def get_square(image, square_size):\n",
    "    \n",
    "    height, width = image.shape    \n",
    "    if(height > width):\n",
    "        differ = height\n",
    "    else:\n",
    "        differ = width\n",
    "    differ += 4\n",
    "\n",
    "\n",
    "    mask = np.zeros((differ, differ), dtype = \"uint8\")\n",
    "\n",
    "    x_pos = int((differ - width) / 2)\n",
    "    y_pos = int((differ - height) / 2)\n",
    "\n",
    "   \n",
    "    mask[y_pos: y_pos + height, x_pos: x_pos + width] = image[0: height, 0: width]\n",
    "\n",
    " \n",
    "    if differ / square_size > 1:\n",
    "        mask = pyramid_reduce(mask, differ / square_size)\n",
    "    else:\n",
    "        mask = cv2.resize(mask, (square_size, square_size), interpolation = cv2.INTER_AREA)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def keras_predict(model, image):\n",
    "    data = np.asarray( image, dtype=\"int32\" )\n",
    "    \n",
    "    pred_probab = model.predict(data)[0]\n",
    "    pred_class = list(pred_probab).index(max(pred_probab))\n",
    "    return max(pred_probab), pred_class\n",
    "\n",
    "def keras_process_image(img):\n",
    "    \n",
    "    image_x = 28\n",
    "    image_y = 28\n",
    "    #img = cv2.resize(img, (28,28), interpolation = cv2.INTER_AREA)\n",
    "    img = get_square(img, 28)\n",
    "    img = np.reshape(img, (image_x, image_y))\n",
    "    \n",
    "    \n",
    "    return img\n",
    " \n",
    "\n",
    "def crop_image(image, x, y, width, height):\n",
    "    return image[y:y + height, x:x + width]\n",
    "\n",
    "while True:  \n",
    "    cam_capture = cv2.VideoCapture(0)\n",
    "    _, image_frame = cam_capture.read()  \n",
    "    # Select ROI\n",
    "    im2 = crop_image(image_frame, 300,300,300,300)\n",
    "    image_grayscale = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n",
    "    image_grayscale_blurred = cv2.GaussianBlur(image_grayscale, (15,15), 0)\n",
    "\n",
    "\n",
    "    #resized_img = image_resize(image_grayscale_blurred, width = 28, height = 28, inter = cv2.INTER_AREA) \n",
    "    #resized_img = keras_process_image(image_grayscale_blurred)\n",
    "    resized_img = cv2.resize(image_grayscale_blurred,(28,28))\n",
    "    #ar = np.array(resized_img)\n",
    "    ar = resized_img.reshape(1,784)\n",
    "\n",
    "    pred_probab, pred_class = keras_predict(model, ar )\n",
    "    print(pred_class, pred_probab)\n",
    "     \n",
    "    \n",
    " \n",
    "    # Display cropped image\n",
    "\n",
    "    cv2.imshow(\"Image2\",im2)\n",
    "    cv2.imshow(\"Image4\",resized_img)\n",
    "    cv2.imshow(\"Image3\",image_grayscale_blurred)\n",
    "\n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "    \n",
    "\n",
    "cam_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
